---
title: "WQD7004 - Red Wine Quality Prediction - Group Project"
author: "Group 6 - Curly Brackets"
output:
  html_document:
    toc: yes
    number_sections: yes
  pdf_document:
    toc: yes
---

# Introduction

Interest in the wine industry is ever increasing, with projected market size growth at USD 456.76 billion in 2028. To meet demands, companies are striving for innovative methods to improve production and sales. A key element to this is quality assurance on wine samples which are largely dependent on tests conducted by human experts for attributes such as taste and flavour which are typical drivers for consumer purchase. While quantitative analysis can be observed through physicochemical tests, sensory tests rely on the subjectivity of human experts and are harder to define/classify. By identifying correlations between analytical data and human preferences, many insights can be derived to improve production and sales efforts. The aim of this project is to create a model to predict wine quality as defined by human experts based on correlations with physicochemical parameters.


## Potential Questions
1. Which features have the highest correlation to quality? 
2. Which is the best classification algorithm for predicting wine quality? 


## Objectives
1. To predict the quality of red wine. 
2. To identify the best model in accuracy of prediction.

# Data Understanding

## Data Source
The wine quality dataset that will be used in this project is obtained from Kaggle
https://www.kaggle.com/datasets/piyushgoyal443/red-wine-dataset?select=wineQualityReds.csv

This data set was originally featured in a research paper by Cortez et al., 2009. 

Citations:

P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.

Modeling wine preferences by data mining from physicochemical properties.
In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.

Available at: [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016
[Pre-press (pdf)] http://www3.dsi.uminho.pt/pcortez/winequality09.pdf
[bib] http://www3.dsi.uminho.pt/pcortez/dss09.bib


## Data Summary
The data set consists of a total of 1600 rows and 12 columns which contains information on the physicochemical parameters and wine quality of 1599 red wine vinho verde samples, from the north of Portugal. The data was collected from May 2004 to February 2007. The purpose of the data set is to determine the quality of wines based on their physicochemical characteristics. 


Variable Name             | Description     | Type |
--------------------------|-----------------| -----
fixed acidity             | amount of non-volatile acids in wine, typically expressed in grams per liter (g/L) | continuous
volatile acidity          | amount of volatile acids in wine, which can contribute to its sourness or vinegar-like taste, typically expressed in g/L | continuous
citric acid               | amount of citric acid, a type of weak organic acid, in wine, typically expressed in g/L | continuous
residual sugar            | amount of sugar left in the wine after fermentation, typically expressed in g/L | continuous
chlorides                 | amount of chloride ions in wine, which can affect its taste and aroma, typically expressed in g/L | continuous
free sulfur dioxide       | amount of sulfur dioxide, a common wine preservative, that is not bound to other chemical compounds, typically expressed in parts per million (ppm) | continuous
total free sulfur dioxide | Represents the total amount of sulfur dioxide, both free and bound, in wine, typically expressed in ppm | continuous
density                   | density of the wine, which can provide information about its body and mouthfeel, typically expressed in g/mL | continuous
pH                        | pH level, which indicates the acidity or basicity  of the wine on a scale from 0 to 14, with lower values indicating higher acidity | continuous
sulphates                 |amount of sulphates, a type of salt, in wine, typically expressed in g/L | continuous
alcohol                   | percentage of alcohol by volume in the wine | continuous
quality                   | quality rating of the wine, which is often given on a scale from 1 to 10, with higher values indicating better quality | discrete

## Importing Dataset & Libraries

```{r}
# Load dataset
wine <- read.csv("wineQualityReds.csv", header = T, sep = ",")
```

```{r, message='FALSE'} 
# install.packages("tidyverse")
library(tidyverse)
# install.packages("dplyr")
library(dplyr)
# install.packages("ggplot2")
library(ggplot2)
# install.packages("kableExtra")
library(kableExtra)
# install.packages("gridExtra")
library(gridExtra)
# install.packages('patchwork')
library(patchwork)
# install.packages("ggcorrplot")
library(ggcorrplot)
# install.packages('glmnet')
# install.packages('caret')
library(glmnet)
library(caret)
```

```{r}
# A summary view of each attributes
summary(wine)
```

```{r}
# head of dataset
head(wine)
```

```{r}
# Dimension of the dataset
dim(wine)
```

```{r}
# Structure of the dataset
str(wine)
```

# Data Cleaning

We perform data sanity check for any dirty data such as missing values. 

```{r}
# Check any missing values
sum(is.na(wine))
colSums(is.na(wine))
```

```{r}
#Check any zero values
sum(wine == 0)
colSums(wine == 0)
```

```{r}
# Check any duplicates
anyDuplicated(wine)
```

```{r}
#Rename Column
wine <- wine %>%
  rename(ID = X,
         Fixed_Acidity = fixed.acidity, 
         Volatile_Acidity = volatile.acidity,
         Citric_Acid = citric.acid, 
         Residual_Sugar = residual.sugar, 
         Chlorides = chlorides, 
         Free_Sulfur_Dioxide = free.sulfur.dioxide, 
         Total_Sulfur_Dioxide = total.sulfur.dioxide, 
         Density = density, 
         pH = pH, 
         Sulphates = sulphates, 
         Alcohol_Percentage = alcohol, 
         Quality = quality)

# Check updated column names
colnames(wine)
```

```{r}
# # Drop column that's not contributing
wine <- wine[, -which(names(wine) == "ID")]

# Check updated columns
colnames(wine)
```
From this preliminary sanity check, 

* There is no missing values in the dataset. Total count of the missing value for each attributes return 0. 
* There are 132 zero values found and all of them are from Citric_Acid. We will find out if these are outliers in the next steps.
* There is no duplicate observations as it return 0 as well. 
* The columns are renamed, and we preview the column names ensure are renamed correctly. 
* ID column dropped due to not contributing towards the project

# Exploratory Data Analysis (EDA)

## Identify Potential Outliers by Plotting Graphs
Box Plot and Histogram gives a veiw of the data distribution. We can then use it to rationalise and identify outliers which required to be removed. 

*   grid.arrange function is used as we are plotting 2 graphs in a page
*   geom_jitter is used for handling overplotting in boxplot
* the limit of is set based on IQR ± 1.5

### Fixed Acidity
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Fixed_Acidity ) ) + 
               geom_jitter(alpha = 0.1) +
               geom_boxplot(alpha = 0.1, color = 'red' ) +
               scale_y_continuous(lim = c(3.95,12.35)),
             ggplot(data = wine, aes(x = Fixed_Acidity)) +
               geom_histogram(binwidth = 1, color = 'black',fill = I('cadetblue2')) + 
               scale_x_continuous(lim = c(3.95,12.35)),ncol = 2)

# list of the outliers
boxplot.stats(wine$Fixed_Acidity)$out
length(boxplot.stats(wine$Fixed_Acidity)$out)
```



Based on IQR ± 1.5 calculation,

Lower Limit is set as 3.95

Upper Limit is set as 12.35

**Conclusion:** 49 outliers with the value greater than 12.35.

### Volatile Acidity
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Volatile_Acidity ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0.015,1.015)),
ggplot(data = wine, aes(x = Volatile_Acidity)) +
  geom_histogram(binwidth = 0.05, color = 'black',fill = I('cadetblue2')) +
  scale_x_continuous(lim = c(0.015,1.015)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Volatile_Acidity)$out
length(boxplot.stats(wine$Volatile_Acidity)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0.015

Upper Limit = 1.015

**Conclusion**: 19 outliers with value greater than 1.015

### Citric Acid 
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Citric_Acid ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0,0.915)),
ggplot(data = wine, aes(x = Citric_Acid)) +
  geom_histogram(binwidth = 0.1, color = 'black',fill = I('cadetblue2')) +
  scale_x_continuous(lim = c(0,0.915)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Citric_Acid)$out
length(boxplot.stats(wine$Citric_Acid)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0

Upper Limit = 0.915

**Conclusion**: Earlier during sanity check, we have identified 132 rows with zero value in Citric_Acid. 1 outliers with value greater than 0.915. 

### Residual Sugar
```{r,warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Residual_Sugar ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0.85,3.65)),
ggplot(data = wine, aes(x = Residual_Sugar)) +
  geom_histogram(binwidth = 0.1, color = 'black',fill = I('cadetblue2')) +
  scale_x_continuous(lim = c(0.85,3.65)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Residual_Sugar)$out
length(boxplot.stats(wine$Residual_Sugar)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0.85. 

Upper Limit = 3.65.

**Conclusion**: 155 rows of outliers with value greater than the upper limit 3.65.

### Chlorides
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Chlorides ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0.04,0.12)),
             ggplot(data = wine, aes(x = Chlorides)) +
               geom_histogram(binwidth = 0.005, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(0.04,0.12)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Chlorides)$out
length(boxplot.stats(wine$Chlorides)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0.04. 

Upper Limit = 0.12.

**Conclusion**: 112 rows of outliers with value falls out between 0.04 to 0.12.

### Free Sulphur Dioxide
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Free_Sulfur_Dioxide ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0,35)),
             ggplot(data = wine, aes(x = Free_Sulfur_Dioxide)) +
               geom_histogram(binwidth = 2, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(0,35)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Free_Sulfur_Dioxide)$out
length(boxplot.stats(wine$Free_Sulfur_Dioxide)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = -14. 

Upper Limit = 35.
Hence, for visualisation purposes, we have set the lower limit to 0. 

**Conclusion**: 30 rows of outliers with value greater than 35.

### Total Sulphur Dioxide
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Total_Sulfur_Dioxide ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0,102)),
             ggplot(data = wine, aes(x = Total_Sulfur_Dioxide)) +
               geom_histogram(binwidth = 5, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(0,102)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Total_Sulfur_Dioxide)$out
length(boxplot.stats(wine$Total_Sulfur_Dioxide)$out)

```

Based on IQR ± 1.5 calculation, 

Lower Limit = -38. 

Upper Limit = 102.
Similarly, we have set the lower limit to 0. 

**Conclusion**: 55 rows of outliers with value greater than 102.

### Density
```{r,warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Density ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0.9923,1)),
             ggplot(data = wine, aes(x = Density)) +
               geom_histogram(binwidth = 0.0005, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(0.9923,1)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Density)$out
length(boxplot.stats(wine$Density)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0.9923. 

Upper Limit = 1.

**Conclusion**: 45 rows of outliers identified.

### pH
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = pH ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(2.925,3.59)),
             ggplot(data = wine, aes(x = pH)) +
               geom_histogram(binwidth = 0.05, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(2.925,3.59)), ncol = 2)

# list of the outliers
boxplot.stats(wine$pH)$out
length(boxplot.stats(wine$pH)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 2.925

Upper Limit = 3.590

**Conclusion**: 35 rows of outliers identified.

### Sulphates
```{r, warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Sulphates) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(0.28,0.91)),
             ggplot(data = wine, aes(x = Sulphates)) +
               geom_histogram(binwidth = 0.05, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(0.28,0.91)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Sulphates)$out
length(boxplot.stats(wine$Sulphates)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 0.28

Upper Limit = 0.91

**Conclusion**: 59 rows of outliers identified, with the value greater than 0.91

### Alcohol Percentage
```{r,warning=FALSE}
grid.arrange(ggplot(wine, aes( x = 1, y = Alcohol_Percentage ) ) + 
               geom_jitter(alpha = 0.1 ) +
               geom_boxplot(alpha = 0.2, color = 'red' ) +
               scale_y_continuous(lim = c(7.1,12.7)),
             ggplot(data = wine, aes(x = Alcohol_Percentage)) +
               geom_histogram(binwidth = 0.5, color = 'black',fill = I('cadetblue2')) +
               scale_x_continuous(lim = c(7.1,12.7)), ncol = 2)

# list of the outliers
boxplot.stats(wine$Alcohol_Percentage)$out
length(boxplot.stats(wine$Alcohol_Percentage)$out)
```

Based on IQR ± 1.5 calculation, 

Lower Limit = 7.1

Upper Limit = 12.7

**Conclusion**: 13 rows of outliers identified, with the value greater than 0.91

## Bivariate Analysis
Analysis of two variables to determine relationships between them.

We will find out the relationship between some wine attributes and the target attribute(wine quality).

```{r}
# Creating boxplots to put into one frame using Patchwork

variables <- colnames(wine)[-12]  # Exclude the "Quality" column

plots <- list()

for (variable in variables) {
  plot <- ggplot(wine) + geom_boxplot(aes(Quality, .data[[variable]], group = Quality))
  plots[[variable]] <- plot
}

combined_plot1 <- wrap_plots(plots[1:4], ncol = 2)
combined_plot2 <- wrap_plots(plots[5:8], ncol = 2)
combined_plot3 <- wrap_plots(plots[9:11], ncol = 2)

```

```{r}
combined_plot1
```

* No significant relation can be seen with Fixed acidity and Quality of wine

* There is negative relationship between wine quality and the volatile acidity. As the quality of wine increases, the volatile acidity decreases.

* As the quality increases, the citric acid content of the wine also increases.

* The median of the residual sugar value for higher quality isn't that much compared to lower quality wine.
So, we cant say that high quality wine has a lot of residual sugar.


```{r}
combined_plot2
```

* No strong relationship between free sulphur dioxide and quality. The median of the free sulphur dioxide of lowest quality and highest quality wine seem to be same. 

* When wine quality increases, density decreases.


```{r}
combined_plot3
```

* There is no significant change in pH as the wine quality increases.

* Sulphates content gets higher as the wine quality increases.

* Wine with better quality seem to have higher alcohol percentage.

## Correlation Analysis

```{r}
cor(wine)
```

```{r}
# Compute a correlation matrix
corr <- round(cor(wine), 1)

# Visualize the correlation matrix
ggcorrplot(corr, hc.order = TRUE, outline.color = "white", lab = TRUE)
```

The correlation matrix reveals the relationships between pairs of variables. Among the variables provided, the top three correlations, in terms of absolute values, are as follows:

*    Quality and Volatile_Acidity: There is a moderate negative correlation (-0.39) between the quality of red wine and the volatile acidity. This indicates that as the volatile acidity increases, the quality of the wine tends to decrease.

*    Alcohol_Percentage and Quality: There is a moderate positive correlation (0.48) between the alcohol percentage and the quality of red wine. This suggests that wines with a higher alcohol percentage tend to have higher quality ratings.

*    Density and Fixed_Acidity: There is a moderate positive correlation (0.67) between the density and fixed acidity of red wine. This suggests that as the fixed acidity increases, the density of the wine tends to increase as well.

## Target Distribution


```{r}
# Count the frequencies of each Quality level
quality_counts <- table(wine$Quality)

# Create a data frame from the frequencies
quality_data <- data.frame(Quality = as.factor(names(quality_counts)),
                           Frequency = as.numeric(quality_counts))

# Generate the pie chart
pie_chart <- ggplot(quality_data, aes(x = "", y = Frequency, fill = Quality)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(fill = "Quality", x = NULL, y = NULL, title = "Quality Distribution") +
  theme_minimal()

# Display the pie chart
pie_chart

```

* The target distribution seems to be mixed up with multiple values. Factoring should be done in pre-processing to divide at least to two values for easier evaluation in modeling section.


# Pre-processing

## Removing Outliers

We have found numerous outliers in EDA section, removing it is necessary for prediction accuracy

```{r}
# Remove Outliers IQR 1.5
remove_outliers <- function(data, column) {
  outliers <- boxplot.stats(data[[column]])$out
  
  data <- data %>%
    filter(!(!!sym(column) %in% outliers))
  
  return(data)
}

# Apply remove_outliers() to each column
columns <- c("Fixed_Acidity", "Volatile_Acidity", "Citric_Acid", "Residual_Sugar", "Chlorides", "Free_Sulfur_Dioxide", "Total_Sulfur_Dioxide")

for (column in columns) {
  fixed_wine <- remove_outliers(wine, column)
}

print(paste0("The cleaned dataset is now with ", nrow(fixed_wine), " records."))

```

## Factoring target values

We had measured during EDA that target values are inconsistent and spread out with multiple values. Factoring it is necessary.

```{r}
fixed_wine$Quality <- factor(ifelse(fixed_wine$Quality %in% c(3, 4, 5), 0, 1))

# Count the frequencies of each Quality category
quality_counts <- table(fixed_wine$Quality)

# Create a data frame from the frequencies
quality_data <- data.frame(Quality_Category = as.factor(names(quality_counts)),
                           Frequency = as.numeric(quality_counts))

# Generate the pie chart
pie_chart <- ggplot(quality_data, aes(x = "", y = Frequency, fill = Quality_Category)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(fill = "Quality Category", x = NULL, y = NULL, title = "Quality Distribution") +
  theme_minimal()

# Display the pie chart
pie_chart

```

## Min-Max

```{r}
numeric_vars <- sapply(fixed_wine, is.numeric)
preproc_obj <- preProcess(fixed_wine[, numeric_vars], method = "range")
fixed_wine[, numeric_vars] <- predict(preproc_obj, fixed_wine[, numeric_vars])
glimpse(fixed_wine)
```

## Balance sample

Here we reconfigure the dataset with ROSE library to prevent imbalance sample.

```{r}
# install.packages('ROSE')
library(ROSE)

# Create possibly balanced samples by random under-sampling using ovun.sample
balanced_sample<-NULL
tmp<-ovun.sample(Quality ~ ., data = fixed_wine, method = "under", p = 0.5, seed = 5)$data
balanced_sample<-rbind(balanced_sample, tmp)
glimpse(balanced_sample)
```

```{r}
summary(balanced_sample$Quality)
```
## Data Partition

```{r}
#Set partition for train and test data
set.seed(123)
training.samples <- balanced_sample$Quality %>% 
  createDataPartition(p = 0.7, list = FALSE)
train.data  <- balanced_sample[training.samples, ]
test.data <- balanced_sample[-training.samples, ]
```



# Modeling



## K-fold Cross-Validation

```{r}
library(caret)

# Set the number of folds (k)
k <- 5

# Create the control object for k-fold cross-validation
ctrl <- trainControl(method = "cv", number = k)

# Logistic Regression
logreg_model <- train(Quality ~ ., data = balanced_sample, method = "glm",
                      trControl = ctrl, family = "binomial")

# Random Forest Classification
rf_model <- train(Quality ~ ., data = balanced_sample, method = "rf",
                  trControl = ctrl)

# Support Vector Machine Classification
svm_model <- train(Quality ~ ., data = balanced_sample, method = "svmRadial",
                   trControl = ctrl)

# Gradient Boosting Classification
gbm_model <- train(Quality ~ ., data = balanced_sample, method = "gbm",
                   trControl = ctrl, verbose = FALSE)

# Results
logreg_cv_results <- logreg_model$results
rf_cv_results <- rf_model$results
svm_cv_results <- svm_model$results
gbm_cv_results <- gbm_model$results
```


```{r}
# Calculate mean and standard deviation of accuracy and kappa
accuracy_mean <- sapply(list(logreg_cv_results, rf_cv_results, svm_cv_results, gbm_cv_results), function(results) mean(results$Accuracy))
accuracy_sd <- sapply(list(logreg_cv_results, rf_cv_results, svm_cv_results, gbm_cv_results), function(results) sd(results$Accuracy))
kappa_mean <- sapply(list(logreg_cv_results, rf_cv_results, svm_cv_results, gbm_cv_results), function(results) mean(results$Kappa))
kappa_sd <- sapply(list(logreg_cv_results, rf_cv_results, svm_cv_results, gbm_cv_results), function(results) sd(results$Kappa))

# Create a data frame to store the summary
summary_df <- data.frame(Model = c("Logistic Regression", "Random Forest", "Support Vector Machine", "Gradient Boosting"),
                         Accuracy_Mean = accuracy_mean,
                         Accuracy_SD = accuracy_sd,
                         Kappa_Mean = kappa_mean,
                         Kappa_SD = kappa_sd)

# Print summary
print(summary_df)

```

## Train-Test Split

```{r}
library(caret)

# Logistic Regression
logreg_model2 <- train(Quality ~ ., data = train.data, method = "glm",
                      family = "binomial")

# Random Forest Classification
rf_model2 <- train(Quality ~ ., data = train.data, method = "rf")

# Support Vector Machine Classification
svm_model2 <- train(Quality ~ ., data = train.data, method = "svmRadial")

# Gradient Boosting Classification
gbm_model2 <- train(Quality ~ ., data = train.data, method = "gbm", verbose = FALSE)

# Obtain the predicted labels for each model
logreg_predictions <- predict(logreg_model2, newdata = test.data)
rf_predictions <- predict(rf_model2, newdata = test.data)
svm_predictions <- predict(svm_model2, newdata = test.data)
gbm_predictions <- predict(gbm_model2, newdata = test.data)

# Create confusion matrix for each model
logreg_confusion <- confusionMatrix(logreg_predictions, test.data$Quality)
rf_confusion <- confusionMatrix(rf_predictions, test.data$Quality)
svm_confusion <- confusionMatrix(svm_predictions, test.data$Quality)
gbm_confusion <- confusionMatrix(gbm_predictions, test.data$Quality)

# Print confusion matrix for each model
print("Logistic Regression Confusion Matrix:")
print(logreg_confusion)
print("Random Forest Confusion Matrix:")
print(rf_confusion)
print("Support Vector Machine Confusion Matrix:")
print(svm_confusion)
print("Gradient Boosting Confusion Matrix:")
print(gbm_confusion)
```


```{r, warning=FALSE}
logreg_matrix <- caret::confusionMatrix(logreg_predictions, test.data$Quality)
logreg_df <- as.data.frame(logreg_matrix$table)
ggplot(logreg_df, aes(x = Reference, y = Prediction, fill = logreg_df$Freq)) +
  geom_tile() +
  geom_text(aes(label = logreg_df$Freq), vjust = 0.5) +
  labs(x = "Reference", y = "Prediction", title = "Logistic Regression Confusion Matrix")

# Plot confusion matrix for Random Forest
rf_matrix <- caret::confusionMatrix(rf_predictions, test.data$Quality)
rf_df <- as.data.frame(rf_matrix$table)
ggplot(rf_df, aes(x = Reference, y = Prediction, fill = rf_df$Freq)) +
  geom_tile() +
  geom_text(aes(label = rf_df$Freq), vjust = 0.5) +
  labs(x = "Reference", y = "Prediction", title = "Random Forest Confusion Matrix")

# Plot confusion matrix for Support Vector Machine
svm_matrix <- caret::confusionMatrix(svm_predictions, test.data$Quality)
svm_df <- as.data.frame(svm_matrix$table)
ggplot(svm_df, aes(x = Reference, y = Prediction, fill = svm_df$Freq)) +
  geom_tile() +
  geom_text(aes(label = svm_df$Freq), vjust = 0.5) +
  labs(x = "Reference", y = "Prediction", title = "Support Vector Machine Confusion Matrix")

# Plot confusion matrix for Gradient Boosting
gbm_matrix <- caret::confusionMatrix(gbm_predictions, test.data$Quality)
gbm_df <- as.data.frame(gbm_matrix$table)
ggplot(gbm_df, aes(x = Reference, y = Prediction, fill = gbm_df$Freq)) +
  geom_tile() +
  geom_text(aes(label = gbm_df$Freq), vjust = 0.5) +
  labs(x = "Reference", y = "Prediction", title = "Gradient Boosting Confusion Matrix")

```


## RF K-Fold Feature Importance

```{r}
# Feature Importance for Random Forest
rf_feature_importance <- varImp(rf_model)
rf_feature_importance
```

```{r}
# Plot the feature importance
plot(rf_feature_importance)

```

## RF Train-Test Feature Importance

```{r}
rf_feature_importance2 <- varImp(rf_model2)
rf_feature_importance2
```

```{r}
# Plot the feature importance
plot(rf_feature_importance2)

```



# Assessment

Based on the assessment of the two approaches (k-fold cross-validation and train-test split), the Random Forest model consistently outperformed the Logistic Regression, Support Vector Machine, and Gradient Boosting models in both approaches.

*    Random Forest:
*       Accuracy (k-fold): 81.03% (with a standard deviation of 0.0036)
*       Kappa (k-fold): 0.6207 (with a standard deviation of 0.0072)
*       Accuracy (train-test split): 80.1%
*       Kappa (train-test split): 0.6018
*    Feature Importance (k-fold):
*       Alcohol Percentage (100% importance)
*       Sulphates (70.06% importance)
*       Volatile Acidity (45.82% importance)
*    Feature Importance (train-test split):
*       Alcohol Percentage (100% importance)
*       Sulphates (60.82% importance)
*       Volatile Acidity (41.33% importance)

The Random Forest model consistently demonstrated superior performance in both the k-fold cross-validation and train-test split approaches, achieving higher accuracy and kappa values compared to the other models.

Regarding feature importance, there are some differences between the two approaches. While Alcohol Percentage, Sulphates, and Volatile Acidity consistently emerged as important features in both approaches, the importance values varied slightly. In the k-fold cross-validation approach, Sulphates had a higher importance value (70.06%), followed by Volatile Acidity (45.82%). In the train-test split approach, Sulphates still ranked high in importance (60.82%), but Volatile Acidity had a lower importance value (41.33%).

The slight differences in feature importance could be attributed to the different subsets of data used in each approach and the inherent variability in the models' learning process. However, the overall trend of the top important features remains consistent across both approaches, emphasizing the significance of Alcohol Percentage, Sulphates, and Volatile Acidity in predicting wine quality.

In conclusion, the Random Forest model consistently outperformed the other models in both approaches, indicating its superiority in predicting wine quality. Although there were slight differences in feature importance between the approaches, the key influential features remained consistent.



# Conclusion
In conclusion, the objective of this analysis was to predict the quality of red wine and identify the best model in terms of prediction accuracy. Two approaches were used to evaluate the models: k-fold cross-validation and train-test split. The following steps were undertaken to achieve these goals:

*    Preprocessing: The dataset was preprocessed, including scaling the features and factorizing the wine quality ratings. Outliers were also removed to prevent bias in the models' predictions.

*    Sample Balancing: Class imbalance was addressed by applying oversampling or undersampling techniques to ensure each wine quality rating had equal representation in the models.

*    Model Evaluation - k-fold cross-validation: Logistic Regression, Random Forest, Support Vector Machine (SVM), and Gradient Boosting models were trained and evaluated using k-fold cross-validation. Mean accuracy and Kappa values were calculated to assess the performance of each model.

*    Model Evaluation - Train-test split: The same models were trained and evaluated using a train-test split approach. Confusion matrices were generated to evaluate the performance of each model.

Based on the results, the Random Forest model consistently outperformed the other models in both approaches. In the k-fold cross-validation approach, Random Forest achieved an accuracy of 81.03% and a Kappa value of 0.6207. In the train-test split approach, Random Forest achieved an accuracy of 80.1% and a Kappa value of 0.6018. These results indicate that the Random Forest model provided the most accurate predictions of red wine quality.

Furthermore, the feature importance analysis revealed that Alcohol Percentage, Sulphates, and Volatile Acidity were consistently identified as the most important features in both approaches. However, there were slight differences in the importance values between the two approaches, suggesting some variability in the feature selection process.

For future work, several opportunities can be explored to further enhance the accuracy and robustness of red wine quality prediction models:

*    Hyperparameter Tuning: Fine-tuning the Random Forest model by optimizing its hyperparameters can potentially improve its performance. Techniques such as grid search or random search can be employed to systematically explore different combinations of hyperparameter values and identify the optimal configuration.

*    Ensemble Models: Ensemble methods, such as bagging or boosting, can be employed to combine the predictions of multiple models. This approach can help mitigate the biases or weaknesses of individual models and improve overall prediction accuracy.

*    Alternative Classification Algorithms: Although the Random Forest model performed well in this analysis, it is worth exploring alternative classification algorithms. Algorithms such as neural networks, gradient boosting machines, or support vector machines with different kernels may offer different perspectives and potentially provide even better performance.

*    Expanded Dataset: Acquiring more data on red wine samples can enhance the diversity and representativeness of the dataset. A larger and more comprehensive dataset can potentially capture additional patterns and relationships, leading to improved model performance and generalizability.

*    External Factors: Exploring the incorporation of external factors, such as weather conditions, grape varieties, or winemaking techniques, can provide additional insights into wine quality prediction. These factors may influence the quality characteristics of red wine and can be considered as additional features in the models.

By pursuing these future works, we can continue to refine and advance the accuracy and reliability of red wine quality prediction models. This can benefit wine producers, sommeliers, and consumers by providing valuable insights into the factors that contribute to high-quality red wines and guiding decision-making processes in the wine industry.

In conclusion, the Random Forest model demonstrated superior performance in predicting the quality of red wine, regardless of the evaluation approach used. The inclusion of feature importance analysis highlighted the consistent significance of Alcohol Percentage, Sulphates, and Volatile Acidity in determining wine quality. By considering these findings, wine producers and enthusiasts can focus on these key factors to improve and maintain the quality of their red wines.

# Team Members Contribution

| Student ID | Name                       | Contribution (Overall Percentage)                 |
|-----------|----------------------------|---------------------------------------------------|
|  S2174905 | Muhammad Azzim Nor Fazilan | Preprocessing/Modeling + Overseeing Project (20%) |
|  S2159403 |        Xiaokai Ma          |                  Modeling (20%)                   |
|  17193844 |  Tarsvini A/P Ravinther    |                  EDA (20%)                        |
|  22061596 |       Kee Kai Sing         |         Data Cleaning + Outliers (20%)            |   
|  S2100146 |        Anis Sofea          |         Intro/Data Understanding (20%)            |

